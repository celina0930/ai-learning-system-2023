import streamlit as st
import sounddevice as sd
import tensorflow as tf
import numpy as np
import threading
import os
import emoji
import pickle
import tempfile
import time
from scipy.io.wavfile import write
from page2 import extract_features

def page3():
    st.title("Emotion Recognition from Speech")
    st.write("ì´ í˜ì´ì§€ì—ì„œëŠ” AIê°€ ë‹¹ì‹ ì˜ ëª©ì†Œë¦¬ì—ì„œ ê°ì •ì„ íŒë‹¨í•˜ë„ë¡ í•©ë‹ˆë‹¤.")

    st.write("í…ŒìŠ¤íŠ¸í•˜ë ¤ëŠ” ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”:")
    uploaded_file = st.file_uploader("Choose an audio file", type=["wav", "mp3"])

    audio_file_path = None
    if "audio_file_path" not in st.session_state:
        st.session_state.audio_file_path = None
        
    if uploaded_file is None:
        st.write("ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ì•„ë˜ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë…¹ìŒì„ ì‹œì‘í•˜ì„¸ìš”.")
        st.markdown('<span style="background-color: yellow">**ì˜¤ëŠ˜ í•˜ë£¨ë¥¼ ë– ì˜¬ë ¤ë³´ë©° ìƒê°ë‚˜ëŠ” ì¼ì„ 5ì´ˆì´ìƒ 10ì´ˆì´í•˜ë¡œ ë§í•´ì£¼ì„¸ìš”.**</span>', unsafe_allow_html=True)

        # Record audio from the microphone
        fs = 44100  # Sample rate
        seconds = 10  # Duration of recording

        if st.button('ğŸ”´ Record'):
            myrecording = np.zeros((fs * seconds, 2))  # Pre-allocate array for the recording

            def record_audio():
                nonlocal myrecording  # Use the array from the outer scope
                recording = sd.rec(int(seconds * fs), samplerate=fs, channels=2)
                sd.wait()  # Wait until recording is finished
                myrecording[:] = recording[:]  # Copy the recording data into the pre-allocated array

            # Start the recording in a separate thread
            record_thread = threading.Thread(target=record_audio)
            record_thread.start()

            # Show a progress bar while recording
            progress_bar = st.progress(0)
            for i in range(seconds):
                # Wait for one second
                time.sleep(1)

                # Update the progress bar
                progress = (i + 1) / seconds
                progress_bar.progress(progress)

            # Wait for the recording thread to finish
            record_thread.join()

            myrecording_int16 = np.clip((myrecording * 32767), -32768, 32767).astype(np.int16)  # Convert to 16-bit data with clipping
            # Save to temporary file and use this as our audio file path
            st.session_state.audio_file_path = 'temp.wav'
            write(st.session_state.audio_file_path, fs, myrecording_int16)
            st.audio(st.session_state.audio_file_path)  # Pass the file path directly to st.audio
    else:
        st.session_state.audio_file_path = os.path.join(tempfile.gettempdir(), uploaded_file.name)
        with open(st.session_state.audio_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

    # ëª¨ë¸ ê²½ë¡œ ì…ë ¥
    st.write("ëª¨ë¸ì´ ì €ì¥ëœ í´ë”ì˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”:")
    model_folder_path = st.text_input("Model folder path", "")

    if not os.path.isdir(model_folder_path):
        st.error("ì…ë ¥ëœ ê²½ë¡œì— í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return

    model_path = os.path.join(model_folder_path, 'saved_model')

    if not os.path.isdir(model_path):
        st.error("ì…ë ¥ëœ í´ë”ì— ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return

    # ëª¨ë¸ ë¡œë“œ
    model = tf.keras.models.load_model(model_path)

    if st.session_state.audio_file_path is not None:
    # ì˜¤ë””ì˜¤ íŒŒì¼ì˜ íŠ¹ì§•ì„ ì¶”ì¶œ
        test_features = extract_features([st.session_state.audio_file_path])
        with open(os.path.join(model_folder_path, 'min_timesteps.pkl'), 'rb') as f:
            min_timesteps = pickle.load(f)
        test_features = test_features[:, :min_timesteps, :]

        # ëª¨ë¸ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
        prediction = model.predict(test_features)
        predicted_class = np.argmax(prediction, axis=1)[0]

        # ë ˆì´ë¸” ë³µì›
        with open(os.path.join(model_folder_path, 'le.pkl'), 'rb') as f: 
            le = pickle.load(f)
        predicted_label = le.inverse_transform([predicted_class])[0]

        # ì´ëª¨ì§€ ë§¤í•‘
        emotion_to_emoji = {
            "ê¸°ì¨": ":joy:",
            "ìƒì²˜": ":cry:",
            "ìŠ¬í””": ":disappointed:",
            "ë¶„ë…¸": ":rage:",
            "ë‹¹í™©": ":flushed:",
            "ë¶ˆì•ˆ": ":worried:"
        }

        if predicted_label not in emotion_to_emoji:
            st.error("ì˜ˆì¸¡ëœ ë ˆì´ë¸”ì— í•´ë‹¹í•˜ëŠ” ì´ëª¨ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return

        predicted_emoji = emoji.emojize(emotion_to_emoji[predicted_label])

        # ê²°ê³¼ ì¶œë ¥
        st.markdown(f"### AIê°€ ë‹¹ì‹ ì˜ ëª©ì†Œë¦¬ì—ì„œ {predicted_label} {predicted_emoji} ê°ì •ì„ ì¸ì‹í•˜ì˜€ìŠµë‹ˆë‹¤.")
    else:
        st.write("ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—…ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì˜¤ë””ì˜¤ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê±°ë‚˜ ë…¹ìŒí•´ì£¼ì„¸ìš”.")
